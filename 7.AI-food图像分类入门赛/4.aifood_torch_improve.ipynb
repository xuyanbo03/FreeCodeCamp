{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "# aifood baseline "}, {"metadata": {}, "cell_type": "markdown", "source": "### \u672cbaseline\u91c7\u7528pytorch\u6846\u67b6\uff0c\u5e94\u7528ModelArts\u7684Notebook\u8fdb\u884c\u5f00\u53d1"}, {"metadata": {}, "cell_type": "markdown", "source": "### \u6570\u636e\u96c6\u83b7\u53d6\n\u5c06\u60a8OBS\u6876\u4e2d\u7684\u6570\u636e\u6587\u4ef6\u52a0\u8f7d\u5230\u6b64notebook\u4e2d\uff0c\u5c06\u5982\u4e0b\u4ee3\u7801\u4e2d\"obs-aifood-baseline\"\u4fee\u6539\u6210\u60a8OBS\u6876\u540d\u79f0\u3002"}, {"metadata": {"scrolled": true, "trusted": false}, "cell_type": "code", "source": "import moxing as mox\nmox.file.copy_parallel('s3://ai-awe-n4/aifood','./aifood/')\nprint(\"done\")", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "INFO:root:Using MoXing-v1.14.0-14d5d81b\nINFO:root:Using OBS-Python-SDK-3.1.2\nINFO:root:Listing OBS: 1000\nINFO:root:Listing OBS: 2000\nINFO:root:Listing OBS: 3000\nINFO:root:Listing OBS: 4000\nINFO:root:Listing OBS: 5000\nINFO:root:pid: None.\t1000/5001\nINFO:root:pid: None.\t2000/5001\nINFO:root:pid: None.\t3000/5001\nINFO:root:pid: None.\t4000/5001\nINFO:root:pid: None.\t5000/5001\n", "name": "stderr"}, {"output_type": "stream", "text": "done\n", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "import moxing as mox\nmox.file.copy('s3://ai-awe-n4/model_output1/model/vgg-resnet-50-torch.pth','./vgg-resnet-50-torch.pth')\nprint(\"done\")", "execution_count": 20, "outputs": [{"output_type": "stream", "text": "done\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "### \u52a0\u8f7d\u4f9d\u8d56"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "\nfrom __future__ import print_function, division\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.autograd import Variable\nimport torchvision\nfrom torchvision import datasets, models, transforms\nimport time\nimport os\n", "execution_count": 1, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### \u52a0\u8f7d\u6570\u636e\u96c6\uff0c\u5e76\u5c06\u5176\u5206\u4e3a\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "dataTrans = transforms.Compose([\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomVerticalFlip(p=0.5),\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.RandomRotation(20),\n#     transforms.FiveCrop(100),\n#     transforms.Lambda(lambda x: transforms.functional.rotate(x, 100)),\n#     transforms.TenCrop((100, 224), vertical_flip=True),\n#     transforms.RandomGrayscale(p=0.3),\n#     transforms.ColorJitter(brightness=0.5, contrast=0.5, hue=0.5),\n#     transforms.Pad(10, padding_mode=\"symmetric\"),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n])\n \n    # image data path\ndata_dir = './aifood/images'\nall_image_datasets = datasets.ImageFolder(data_dir, dataTrans)\n#print(all_image_datasets.class_to_idx)    \ntrainsize = int(0.7*len(all_image_datasets))\ntestsize = len(all_image_datasets) - trainsize\ntrain_dataset, test_dataset = torch.utils.data.random_split(all_image_datasets,[trainsize,testsize])\n   \nimage_datasets = {'train':train_dataset,'val':test_dataset}\n    \n\n    # wrap your data and label into Tensor\n\n    \ndataloders = {x: torch.utils.data.DataLoader(image_datasets[x],\n                                                 batch_size=64,\n                                                 shuffle=True,\n                                                 num_workers=4) for x in ['train', 'val']}\n\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n\n    # use gpu or not\nuse_gpu = torch.cuda.is_available()", "execution_count": 13, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def train_model(model, lossfunc, optimizer, scheduler, num_epochs=10):\n    start_time = time.time()\n\n    best_model_wts = model.state_dict()\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n        \n        if epoch % 10 == 0:\n            # random dataset\n            train_dataset, test_dataset = torch.utils.data.random_split(all_image_datasets,[trainsize,testsize])\n            image_datasets = {'train':train_dataset,'val':test_dataset}\n            # wrap your data and label into Tensor\n            dataloders = {x: torch.utils.data.DataLoader(image_datasets[x],batch_size=64,shuffle=True,\n                                                         num_workers=4) for x in ['train', 'val']}\n            dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                scheduler.step()\n                model.train(True)  # Set model to training mode\n            else:\n                model.train(False)  # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0.0\n\n            # Iterate over data.\n            for data in dataloders[phase]:\n                # get the inputs\n                inputs, labels = data\n                \n\n                # wrap them in Variable\n                if use_gpu:\n                    inputs = Variable(inputs.cuda())\n                    labels = Variable(labels.cuda())\n                else:\n                    inputs, labels = Variable(inputs), Variable(labels)\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                outputs = model(inputs)\n                _, preds = torch.max(outputs.data, 1)\n                loss = lossfunc(outputs, labels)\n\n                # backward + optimize only if in training phase\n                if phase == 'train':\n                    loss.backward()\n                    optimizer.step()\n\n                # statistics\n                running_loss += loss.data\n                running_corrects += torch.sum(preds == labels.data).to(torch.float32)\n\n            epoch_loss = running_loss / dataset_sizes[phase]\n            epoch_acc = running_corrects / dataset_sizes[phase]\n\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n                phase, epoch_loss, epoch_acc))\n\n            # deep copy the model\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = model.state_dict()\n\n    elapsed_time = time.time() - start_time\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        elapsed_time // 60, elapsed_time % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n  \n    return model", "execution_count": 17, "outputs": []}, {"metadata": {"scrolled": true, "trusted": true}, "cell_type": "code", "source": "# get model and replace the original fc layer with your fc layer\nmodel_ft = models.resnet50(pretrained=True)\nnum_ftrs = model_ft.fc.in_features\nmodel_ft.fc = nn.Linear(num_ftrs, 10)\n\nif use_gpu:\n    model_ft = model_ft.cuda()\n\n    # define loss function\nlossfunc = nn.CrossEntropyLoss()\n\n    # setting optimizer and trainable parameters\n #   params = model_ft.parameters()\n # list(model_ft.fc.parameters())+list(model_ft.layer4.parameters())\n#params = list(model_ft.fc.parameters())+list( model_ft.parameters())\nparams = list(model_ft.fc.parameters())\noptimizer_ft = optim.SGD(params, lr=0.001, momentum=0.9)\n\n    # Decay LR by a factor of 0.1 every 7 epochs\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=5, gamma=0.1)\n\nmodel_ft.load_state_dict(torch.load('params.pkl'))\nmodel_ft = train_model(model=model_ft,\n                       lossfunc=lossfunc,\n                       optimizer=optimizer_ft,\n                       scheduler=exp_lr_scheduler,\n                       num_epochs=30)", "execution_count": 15, "outputs": [{"output_type": "stream", "text": "Epoch 0/29\n----------\ntrain Loss: 0.0093 Acc: 0.8280\nval Loss: 0.0095 Acc: 0.8300\nEpoch 1/29\n----------\ntrain Loss: 0.0091 Acc: 0.8406\nval Loss: 0.0094 Acc: 0.8320\nEpoch 2/29\n----------\ntrain Loss: 0.0091 Acc: 0.8303\nval Loss: 0.0089 Acc: 0.8353\nEpoch 3/29\n----------\ntrain Loss: 0.0087 Acc: 0.8351\nval Loss: 0.0096 Acc: 0.8313\nEpoch 4/29\n----------\ntrain Loss: 0.0083 Acc: 0.8509\nval Loss: 0.0091 Acc: 0.8333\nEpoch 5/29\n----------\ntrain Loss: 0.0082 Acc: 0.8474\nval Loss: 0.0091 Acc: 0.8327\nEpoch 7/29\n----------\ntrain Loss: 0.0083 Acc: 0.8454\nval Loss: 0.0091 Acc: 0.8393\nEpoch 8/29\n----------\ntrain Loss: 0.0082 Acc: 0.8486\nval Loss: 0.0089 Acc: 0.8347\nEpoch 9/29\n----------\ntrain Loss: 0.0083 Acc: 0.8511\nval Loss: 0.0091 Acc: 0.8380\nEpoch 10/29\n----------\ntrain Loss: 0.0081 Acc: 0.8597\nval Loss: 0.0090 Acc: 0.8320\nEpoch 11/29\n----------\ntrain Loss: 0.0082 Acc: 0.8443\nval Loss: 0.0090 Acc: 0.8287\nEpoch 12/29\n----------\ntrain Loss: 0.0081 Acc: 0.8440\nval Loss: 0.0091 Acc: 0.8447\nEpoch 13/29\n----------\ntrain Loss: 0.0083 Acc: 0.8437\nval Loss: 0.0090 Acc: 0.8313\nEpoch 14/29\n----------\ntrain Loss: 0.0081 Acc: 0.8531\nval Loss: 0.0090 Acc: 0.8340\nEpoch 15/29\n----------\ntrain Loss: 0.0083 Acc: 0.8511\nval Loss: 0.0087 Acc: 0.8473\nEpoch 16/29\n----------\ntrain Loss: 0.0082 Acc: 0.8523\nval Loss: 0.0091 Acc: 0.8320\nEpoch 17/29\n----------\ntrain Loss: 0.0083 Acc: 0.8443\nval Loss: 0.0091 Acc: 0.8480\nEpoch 18/29\n----------\ntrain Loss: 0.0083 Acc: 0.8469\nval Loss: 0.0091 Acc: 0.8433\nEpoch 19/29\n----------\ntrain Loss: 0.0083 Acc: 0.8449\nval Loss: 0.0088 Acc: 0.8380\nEpoch 20/29\n----------\ntrain Loss: 0.0081 Acc: 0.8529\nval Loss: 0.0090 Acc: 0.8333\nEpoch 21/29\n----------\ntrain Loss: 0.0083 Acc: 0.8466\nval Loss: 0.0088 Acc: 0.8493\nEpoch 22/29\n----------\ntrain Loss: 0.0082 Acc: 0.8414\nval Loss: 0.0090 Acc: 0.8407\nEpoch 23/29\n----------\ntrain Loss: 0.0083 Acc: 0.8460\nval Loss: 0.0088 Acc: 0.8420\nEpoch 24/29\n----------\ntrain Loss: 0.0081 Acc: 0.8534\nval Loss: 0.0089 Acc: 0.8367\nEpoch 25/29\n----------\ntrain Loss: 0.0081 Acc: 0.8509\nval Loss: 0.0090 Acc: 0.8293\nEpoch 26/29\n----------\ntrain Loss: 0.0082 Acc: 0.8414\nval Loss: 0.0091 Acc: 0.8373\nEpoch 27/29\n----------\ntrain Loss: 0.0081 Acc: 0.8529\nval Loss: 0.0089 Acc: 0.8407\nEpoch 28/29\n----------\ntrain Loss: 0.0082 Acc: 0.8440\nval Loss: 0.0092 Acc: 0.8313\nEpoch 29/29\n----------\ntrain Loss: 0.0082 Acc: 0.8434\nval Loss: 0.0092 Acc: 0.8340\nTraining complete in 11m 20s\nBest val Acc: 0.849333\n", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "torch.save(model_ft.state_dict(), './model.pth')", "execution_count": 10, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "import moxing as mox\nmox.file.copy('./model.pth','s3://ai-awe-n4/model_output3/model/resnet-50-torch.pth')\nprint(\"done\")", "execution_count": 16, "outputs": [{"output_type": "stream", "text": "INFO:root:Using MoXing-v1.14.0-14d5d81b\nINFO:root:Using OBS-Python-SDK-3.1.2\n", "name": "stderr"}, {"output_type": "stream", "text": "done\n", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "params = list(model_ft.fc.parameters())\noptimizer_ft = optim.Adam(params, lr = 0.00001,weight_decay=0.1)\n\n    # Decay LR by a factor of 0.1 every 7 epochs\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n\nmodel_ft.load_state_dict(torch.load('model.pth'))\nmodel_ft = train_model(model=model_ft,\n                       lossfunc=lossfunc,\n                       optimizer=optimizer_ft,\n                       scheduler=exp_lr_scheduler,\n                       num_epochs=30)", "execution_count": 30, "outputs": [{"output_type": "stream", "text": "Epoch 0/29\n----------\ntrain Loss: 0.0096 Acc: 0.8303\nval Loss: 0.0088 Acc: 0.8507\nEpoch 1/29\n----------\ntrain Loss: 0.0094 Acc: 0.8380\nval Loss: 0.0090 Acc: 0.8400\nEpoch 2/29\n----------\ntrain Loss: 0.0097 Acc: 0.8257\nval Loss: 0.0090 Acc: 0.8380\nEpoch 3/29\n----------\ntrain Loss: 0.0094 Acc: 0.8403\nval Loss: 0.0089 Acc: 0.8400\nEpoch 4/29\n----------\ntrain Loss: 0.0095 Acc: 0.8369\nval Loss: 0.0089 Acc: 0.8527\nEpoch 5/29\n----------\ntrain Loss: 0.0096 Acc: 0.8320\nval Loss: 0.0089 Acc: 0.8473\nEpoch 6/29\n----------\ntrain Loss: 0.0095 Acc: 0.8311\nval Loss: 0.0089 Acc: 0.8487\nEpoch 7/29\n----------\ntrain Loss: 0.0096 Acc: 0.8357\nval Loss: 0.0090 Acc: 0.8480\nEpoch 8/29\n----------\ntrain Loss: 0.0096 Acc: 0.8297\nval Loss: 0.0089 Acc: 0.8460\nEpoch 9/29\n----------\ntrain Loss: 0.0094 Acc: 0.8397\nval Loss: 0.0091 Acc: 0.8413\nEpoch 10/29\n----------\ntrain Loss: 0.0094 Acc: 0.8266\nval Loss: 0.0092 Acc: 0.8400\nEpoch 11/29\n----------\ntrain Loss: 0.0094 Acc: 0.8351\nval Loss: 0.0094 Acc: 0.8353\nEpoch 12/29\n----------\ntrain Loss: 0.0097 Acc: 0.8286\nval Loss: 0.0090 Acc: 0.8513\nEpoch 13/29\n----------\ntrain Loss: 0.0095 Acc: 0.8303\nval Loss: 0.0091 Acc: 0.8513\nEpoch 14/29\n----------\ntrain Loss: 0.0096 Acc: 0.8334\nval Loss: 0.0090 Acc: 0.8487\nEpoch 15/29\n----------\ntrain Loss: 0.0094 Acc: 0.8346\nval Loss: 0.0091 Acc: 0.8380\nEpoch 16/29\n----------\ntrain Loss: 0.0096 Acc: 0.8320\nval Loss: 0.0093 Acc: 0.8420\nEpoch 17/29\n----------\ntrain Loss: 0.0094 Acc: 0.8323\nval Loss: 0.0092 Acc: 0.8387\nEpoch 18/29\n----------\ntrain Loss: 0.0097 Acc: 0.8280\nval Loss: 0.0091 Acc: 0.8447\nEpoch 19/29\n----------\ntrain Loss: 0.0094 Acc: 0.8329\nval Loss: 0.0092 Acc: 0.8393\nEpoch 20/29\n----------\ntrain Loss: 0.0094 Acc: 0.8334\nval Loss: 0.0094 Acc: 0.8327\nEpoch 21/29\n----------\ntrain Loss: 0.0093 Acc: 0.8354\nval Loss: 0.0094 Acc: 0.8420\nEpoch 22/29\n----------\ntrain Loss: 0.0095 Acc: 0.8266\nval Loss: 0.0093 Acc: 0.8360\nEpoch 23/29\n----------\ntrain Loss: 0.0094 Acc: 0.8377\nval Loss: 0.0093 Acc: 0.8380\nEpoch 24/29\n----------\ntrain Loss: 0.0094 Acc: 0.8289\nval Loss: 0.0093 Acc: 0.8493\nEpoch 25/29\n----------\ntrain Loss: 0.0093 Acc: 0.8334\nval Loss: 0.0093 Acc: 0.8440\nEpoch 26/29\n----------\ntrain Loss: 0.0092 Acc: 0.8371\nval Loss: 0.0094 Acc: 0.8447\nEpoch 27/29\n----------\ntrain Loss: 0.0095 Acc: 0.8331\nval Loss: 0.0095 Acc: 0.8360\nEpoch 28/29\n----------\ntrain Loss: 0.0094 Acc: 0.8294\nval Loss: 0.0095 Acc: 0.8440\nEpoch 29/29\n----------\ntrain Loss: 0.0095 Acc: 0.8306\nval Loss: 0.0096 Acc: 0.8373\nTraining complete in 11m 18s\nBest val Acc: 0.852667\n", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "torch.save(model_ft.state_dict(), './model-resnet.pth')", "execution_count": 31, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "import moxing as mox\nmox.file.copy('./model-resnet.pth','s3://ai-awe-n4/model_output5/model/resnet-50-torch.pth')\nprint(\"done\")", "execution_count": 32, "outputs": [{"output_type": "stream", "text": "done\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "### VGG"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# \u6784\u5efa\u591a\u6a21\u578b\u878d\u5408\u7ed3\u6784\nmodel_1 = models.vgg16(pretrained = False)\n\n# \u8bbe\u7f6e\u6a21\u578b\u7684\u53c2\u6570\u4e0d\u9700\u8981\u8fdb\u884c\u68af\u5ea6\u4e0b\u964d\nfor param in model_1.parameters():\n    param.requires_grad = False\nmodel_1.classifier = torch.nn.Sequential(torch.nn.Linear(25088,4096),\n                                        torch.nn.ReLU(),\n                                        torch.nn.Dropout(p = 0.5),\n                                        torch.nn.Linear(4096,4096),\n                                        torch.nn.ReLU(),\n                                        torch.nn.Dropout(p = 0.5),\n                                        torch.nn.Linear(4096,10))\n\nUse_gpu = torch.cuda.is_available()\n# \u8bbe\u7f6e\u635f\u5931\u51fd\u6570\u4ee5\u53ca\u4f18\u5316\u65b9\u6cd5\nif Use_gpu:\n    model_1 = model_1.cuda()\n    \nloss_f_1 = torch.nn.CrossEntropyLoss()\noptimizer_1 = torch.optim.Adam(model_1.classifier.parameters(), lr = 0.00001,weight_decay=0.1)\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_1, step_size=7, gamma=0.1)", "execution_count": 19, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "load_name = './vgg-resnet-50-torch.pth'\ncheckpoint = torch.load(load_name,map_location ='cpu')\nmodel_1.load_state_dict(checkpoint['model_1_state_dict'])\n# model_2.load_state_dict(checkpoint['model_2_state_dict'])\nmodel_1.eval()", "execution_count": 21, "outputs": [{"output_type": "execute_result", "execution_count": 21, "data": {"text/plain": "VGG(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace)\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace)\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (6): ReLU(inplace)\n    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): ReLU(inplace)\n    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace)\n    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (13): ReLU(inplace)\n    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (15): ReLU(inplace)\n    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (18): ReLU(inplace)\n    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (20): ReLU(inplace)\n    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (22): ReLU(inplace)\n    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (25): ReLU(inplace)\n    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (27): ReLU(inplace)\n    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (29): ReLU(inplace)\n    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (classifier): Sequential(\n    (0): Linear(in_features=25088, out_features=4096, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.5)\n    (3): Linear(in_features=4096, out_features=4096, bias=True)\n    (4): ReLU()\n    (5): Dropout(p=0.5)\n    (6): Linear(in_features=4096, out_features=10, bias=True)\n  )\n)"}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "optimizer_1 = torch.optim.Adam(model_1.classifier.parameters(), lr = 0.000001)\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_1, step_size=7, gamma=0.1)", "execution_count": 24, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "model_1 = train_model(model=model_1,\n                       lossfunc=loss_f_1,\n                       optimizer=optimizer_1,\n                       scheduler=exp_lr_scheduler,\n                       num_epochs=20)", "execution_count": 27, "outputs": [{"output_type": "stream", "text": "Epoch 0/19\n----------\ntrain Loss: 0.0058 Acc: 0.8894\nval Loss: 0.0053 Acc: 0.9007\nEpoch 1/19\n----------\ntrain Loss: 0.0060 Acc: 0.8791\nval Loss: 0.0052 Acc: 0.9127\nEpoch 2/19\n----------\ntrain Loss: 0.0059 Acc: 0.8840\nval Loss: 0.0051 Acc: 0.9193\nEpoch 3/19\n----------\ntrain Loss: 0.0059 Acc: 0.8926\nval Loss: 0.0050 Acc: 0.9133\nEpoch 4/19\n----------\ntrain Loss: 0.0060 Acc: 0.8869\nval Loss: 0.0052 Acc: 0.9060\nEpoch 5/19\n----------\ntrain Loss: 0.0057 Acc: 0.8874\nval Loss: 0.0051 Acc: 0.9073\nEpoch 6/19\n----------\ntrain Loss: 0.0060 Acc: 0.8886\nval Loss: 0.0052 Acc: 0.9020\nEpoch 7/19\n----------\ntrain Loss: 0.0060 Acc: 0.8823\nval Loss: 0.0052 Acc: 0.9013\nEpoch 8/19\n----------\ntrain Loss: 0.0060 Acc: 0.8791\nval Loss: 0.0053 Acc: 0.9007\nEpoch 9/19\n----------\ntrain Loss: 0.0060 Acc: 0.8849\nval Loss: 0.0050 Acc: 0.9060\nEpoch 10/19\n----------\ntrain Loss: 0.0057 Acc: 0.8911\nval Loss: 0.0054 Acc: 0.8947\nEpoch 11/19\n----------\ntrain Loss: 0.0059 Acc: 0.8891\nval Loss: 0.0056 Acc: 0.8887\nEpoch 12/19\n----------\ntrain Loss: 0.0058 Acc: 0.8860\nval Loss: 0.0053 Acc: 0.9013\nEpoch 13/19\n----------\ntrain Loss: 0.0059 Acc: 0.8831\nval Loss: 0.0055 Acc: 0.8973\nEpoch 14/19\n----------\ntrain Loss: 0.0058 Acc: 0.8877\nval Loss: 0.0055 Acc: 0.8987\nEpoch 15/19\n----------\ntrain Loss: 0.0059 Acc: 0.8886\nval Loss: 0.0053 Acc: 0.9027\nEpoch 16/19\n----------\ntrain Loss: 0.0059 Acc: 0.8817\nval Loss: 0.0052 Acc: 0.9040\nEpoch 17/19\n----------\ntrain Loss: 0.0058 Acc: 0.8889\nval Loss: 0.0055 Acc: 0.8927\nEpoch 18/19\n----------\ntrain Loss: 0.0057 Acc: 0.8900\nval Loss: 0.0051 Acc: 0.9047\nEpoch 19/19\n----------\ntrain Loss: 0.0059 Acc: 0.8894\nval Loss: 0.0056 Acc: 0.8987\nTraining complete in 5m 26s\nBest val Acc: 0.919333\n", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "torch.save(model_ft.state_dict(), './model-vgg.pth')", "execution_count": 28, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "import moxing as mox\nmox.file.copy('./model-vgg.pth','s3://ai-awe-n4/model_output4/model/vgg-16-torch.pth')\nprint(\"done\")", "execution_count": 29, "outputs": [{"output_type": "stream", "text": "done\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "### \u6a21\u578b\u8bad\u7ec3\n\u91c7\u7528resnet50\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\u8bad\u7ec3\u6a21\u578b,\u6a21\u578b\u8bad\u7ec3\u9700\u8981\u4e00\u5b9a\u65f6\u95f4\uff0c\u7b49\u5f85\u8be5\u6bb5\u4ee3\u7801\u8fd0\u884c\u5b8c\u6210\u540e\u518d\u5f80\u4e0b\u6267\u884c\u3002"}, {"metadata": {"trusted": false}, "cell_type": "code", "source": "# \u6784\u5efa\u591a\u6a21\u578b\u878d\u5408\u7ed3\u6784\nmodel_1 = models.vgg16(pretrained = False)\nmodel_2 = models.resnet50(pretrained = True)\n\n# \u8bbe\u7f6e\u6a21\u578b\u7684\u53c2\u6570\u4e0d\u9700\u8981\u8fdb\u884c\u68af\u5ea6\u4e0b\u964d\nfor param in model_1.parameters():\n    param.requires_grad = False\nmodel_1.classifier = torch.nn.Sequential(torch.nn.Linear(25088,4096),\n                                        torch.nn.ReLU(),\n                                        torch.nn.Dropout(p = 0.5),\n                                        torch.nn.Linear(4096,4096),\n                                        torch.nn.ReLU(),\n                                        torch.nn.Dropout(p = 0.5),\n                                        torch.nn.Linear(4096,10))\nfor param in model_2.parameters():\n    param.requires_grad = False\nnum_ftrs = model_2.fc.in_features\nmodel_2.fc = torch.nn.Linear(num_ftrs,10)\n\n\nUse_gpu = torch.cuda.is_available()\n# \u8bbe\u7f6e\u635f\u5931\u51fd\u6570\u4ee5\u53ca\u4f18\u5316\u65b9\u6cd5\nif Use_gpu:\n    model_1 = model_1.cuda()\n    model_2 = model_2.cuda()\n    \nloss_f_1 = torch.nn.CrossEntropyLoss()\nloss_f_2 = torch.nn.CrossEntropyLoss()\n\noptimizer_1 = torch.optim.Adam(model_1.classifier.parameters(), lr = 0.00001,weight_decay=0.1)\nparams_2 = list(model_2.fc.parameters())\noptimizer_2 = optim.SGD(params_2, lr=0.001, momentum=0.9)\n\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_2, step_size=7, gamma=0.1)\n\n# \u8bbe\u7f6e\u4e24\u4e2a\u6a21\u578b\u878d\u5408\u7684\u6743\u91cd\u53c2\u6570\nweight_1 = 0.4\nweight_2 = 0.6", "execution_count": 15, "outputs": []}, {"metadata": {"trusted": false}, "cell_type": "code", "source": "load_name = './vgg-resnet-50-torch.pth'\ncheckpoint = torch.load(load_name,map_location ='cpu')\nmodel_1.load_state_dict(checkpoint['model_1_state_dict'])\n# model_2.load_state_dict(checkpoint['model_2_state_dict'])\nmodel_1.eval()\n# model_2.eval()", "execution_count": 16, "outputs": [{"output_type": "execute_result", "execution_count": 16, "data": {"text/plain": "VGG(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace)\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace)\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (6): ReLU(inplace)\n    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): ReLU(inplace)\n    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace)\n    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (13): ReLU(inplace)\n    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (15): ReLU(inplace)\n    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (18): ReLU(inplace)\n    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (20): ReLU(inplace)\n    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (22): ReLU(inplace)\n    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (25): ReLU(inplace)\n    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (27): ReLU(inplace)\n    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (29): ReLU(inplace)\n    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (classifier): Sequential(\n    (0): Linear(in_features=25088, out_features=4096, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.5)\n    (3): Linear(in_features=4096, out_features=4096, bias=True)\n    (4): ReLU()\n    (5): Dropout(p=0.5)\n    (6): Linear(in_features=4096, out_features=10, bias=True)\n  )\n)"}, "metadata": {}}]}, {"metadata": {"trusted": false}, "cell_type": "code", "source": "def train_model_cross(model_1,model_2, loss_f_1,loss_f_2, optimizer_1,optimizer_2,weight_1,weight_2, scheduler, num_epochs=10):\n    start_time = time.time()\n\n    best_model_wts_1 = model_1.state_dict()\n    best_model_wts_2 = model_2.state_dict()\n    best_acc = 0.0\n    \n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n        \n        if epoch % 5 == 0:\n            # random dataset\n            train_dataset, test_dataset = torch.utils.data.random_split(all_image_datasets,[trainsize,testsize])\n            image_datasets = {'train':train_dataset,'val':test_dataset}\n            # wrap your data and label into Tensor\n            dataloders = {x: torch.utils.data.DataLoader(image_datasets[x],batch_size=64,shuffle=True,\n                                                         num_workers=4) for x in ['train', 'val']}\n            dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                scheduler.step()\n                model_1.train(True)\n                model_2.train(True)  # Set model to training mode\n            else:\n                model_1.train(False)\n                model_2.train(False)  # Set model to evaluate mode\n\n            running_loss_1 = 0.0\n            running_loss_2 = 0.0\n            running_corrects_1 = 0.0\n            running_corrects_2 = 0.0\n            blending_running_corrects = 0.0\n            \n            # Iterate over data.\n            for data in dataloders[phase]:\n                # get the inputs\n                inputs, labels = data\n                \n\n                # wrap them in Variable\n                if use_gpu:\n                    inputs = Variable(inputs.cuda())\n                    labels = Variable(labels.cuda())\n                else:\n                    inputs, labels = Variable(inputs), Variable(labels)\n\n\n                # forward\n                y_pred_1 = model_1(inputs)\n                y_pred_2 = model_2(inputs)\n#                 out = torch.cat((y_pred_1,y_pred_2),1)\n                blending_y_pred = y_pred_1 * weight_1 + y_pred_2 * weight_2\n                _, pred_1 = torch.max(y_pred_1.data,1) # \u627e\u51fa\u6bcf\u4e00\u884c\u6700\u5927\u503c\u5bf9\u5e94\u7684\u7d22\u5f15\u503c\n                _, pred_2 = torch.max(y_pred_2.data,1)\n                _, blending_y_pred = torch.max(blending_y_pred.data,1)\n                \n                optimizer_1.zero_grad()\n                optimizer_2.zero_grad()\n\n                loss_1 = loss_f_1(y_pred_1,labels)\n                loss_2 = loss_f_2(y_pred_2,labels)\n                \n\n                # backward + optimize only if in training phase\n                if phase == 'train':\n                    loss_1.backward()\n                    loss_2.backward()\n                    optimizer_1.step()\n                    optimizer_2.step()\n                    \n                # statistics\n                running_loss_1 += loss_1.data\n                running_loss_2 += loss_2.data\n                running_corrects_1 += torch.sum(pred_1 == labels.data).to(torch.float32)\n                running_corrects_2 += torch.sum(pred_2 == labels.data).to(torch.float32)\n                blending_running_corrects += torch.sum(blending_y_pred == labels.data).to(torch.float32)\n\n            \n            epoch_loss_1 = running_loss_1/dataset_sizes[phase]\n            epoch_acc_1 = running_corrects_1 * 100/dataset_sizes[phase]\n            epoch_loss_2 = running_loss_2/dataset_sizes[phase]\n            epoch_acc_2 = running_corrects_2 * 100/dataset_sizes[phase]\n            epoch_blending_acc = blending_running_corrects * 100/dataset_sizes[phase]\n\n            print('{}, Model1 Loss:{:.4f},Model1 ACC:{:.4f}%,Model2 Loss:{:.4f},Model2 ACC:{:.4f}%,Blending_Model ACC:{:.4f}'\n                  .format(phase,epoch_loss_1,epoch_acc_1,epoch_loss_2,epoch_acc_2,epoch_blending_acc))\n\n            # deep copy the model\n            if phase == 'val' and epoch_blending_acc > best_acc:\n                best_acc = epoch_blending_acc\n                best_model_wts_1 = model_1.state_dict()\n                best_model_wts_2 = model_2.state_dict()\n\n    elapsed_time = time.time() - start_time\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        elapsed_time // 60, elapsed_time % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n\n    # load best model weights\n    model_1.load_state_dict(best_model_wts_1)\n    model_2.load_state_dict(best_model_wts_2)\n  \n    return model_1,model_2", "execution_count": 20, "outputs": []}, {"metadata": {"trusted": false}, "cell_type": "code", "source": "model_1,model_2 = train_model_cross(\n    model_1,model_2, loss_f_1,loss_f_2, optimizer_1,optimizer_2,weight_1,weight_2, exp_lr_scheduler, num_epochs=40)", "execution_count": 21, "outputs": [{"output_type": "stream", "text": "Epoch 0/39\n----------\ntrain, Model1 Loss:0.0097,Model1 ACC:80.8000%,Model2 Loss:0.0306,Model2 ACC:43.0000%,Blending_Model ACC:81.4286\nval, Model1 Loss:0.0073,Model1 ACC:85.6667%,Model2 Loss:0.0245,Model2 ACC:68.3333%,Blending_Model ACC:86.6667\nEpoch 1/39\n----------\ntrain, Model1 Loss:0.0080,Model1 ACC:83.6286%,Model2 Loss:0.0206,Model2 ACC:73.3714%,Blending_Model ACC:85.3429\nval, Model1 Loss:0.0068,Model1 ACC:86.3333%,Model2 Loss:0.0184,Model2 ACC:75.0667%,Blending_Model ACC:86.8000\nEpoch 2/39\n----------\ntrain, Model1 Loss:0.0073,Model1 ACC:85.6000%,Model2 Loss:0.0161,Model2 ACC:78.3143%,Blending_Model ACC:87.2857\nval, Model1 Loss:0.0064,Model1 ACC:86.4000%,Model2 Loss:0.0151,Model2 ACC:79.3333%,Blending_Model ACC:86.9333\nEpoch 3/39\n----------\ntrain, Model1 Loss:0.0066,Model1 ACC:86.8000%,Model2 Loss:0.0139,Model2 ACC:79.5714%,Blending_Model ACC:88.8000\nval, Model1 Loss:0.0066,Model1 ACC:86.4667%,Model2 Loss:0.0137,Model2 ACC:79.0667%,Blending_Model ACC:86.4000\nEpoch 4/39\n----------\ntrain, Model1 Loss:0.0067,Model1 ACC:86.9714%,Model2 Loss:0.0127,Model2 ACC:80.6571%,Blending_Model ACC:88.0571\nval, Model1 Loss:0.0063,Model1 ACC:87.8667%,Model2 Loss:0.0123,Model2 ACC:80.5333%,Blending_Model ACC:88.4667\nEpoch 5/39\n----------\ntrain, Model1 Loss:0.0070,Model1 ACC:85.7143%,Model2 Loss:0.0118,Model2 ACC:81.0000%,Blending_Model ACC:87.7714\nval, Model1 Loss:0.0063,Model1 ACC:88.7333%,Model2 Loss:0.0117,Model2 ACC:81.1333%,Blending_Model ACC:88.6667\nEpoch 6/39\n----------\ntrain, Model1 Loss:0.0066,Model1 ACC:87.1714%,Model2 Loss:0.0113,Model2 ACC:80.9429%,Blending_Model ACC:88.9714\nval, Model1 Loss:0.0063,Model1 ACC:88.1333%,Model2 Loss:0.0111,Model2 ACC:82.4667%,Blending_Model ACC:88.6000\nEpoch 7/39\n----------\ntrain, Model1 Loss:0.0067,Model1 ACC:87.3429%,Model2 Loss:0.0112,Model2 ACC:81.9714%,Blending_Model ACC:88.5143\nval, Model1 Loss:0.0063,Model1 ACC:88.4667%,Model2 Loss:0.0113,Model2 ACC:82.6000%,Blending_Model ACC:88.8667\nEpoch 8/39\n----------\ntrain, Model1 Loss:0.0065,Model1 ACC:87.5429%,Model2 Loss:0.0111,Model2 ACC:80.9714%,Blending_Model ACC:88.6571\nval, Model1 Loss:0.0067,Model1 ACC:87.6000%,Model2 Loss:0.0110,Model2 ACC:82.4000%,Blending_Model ACC:89.0667\nEpoch 9/39\n----------\ntrain, Model1 Loss:0.0064,Model1 ACC:88.2000%,Model2 Loss:0.0111,Model2 ACC:81.6000%,Blending_Model ACC:89.1143\nval, Model1 Loss:0.0068,Model1 ACC:86.2667%,Model2 Loss:0.0111,Model2 ACC:81.9333%,Blending_Model ACC:87.2667\nEpoch 10/39\n----------\ntrain, Model1 Loss:0.0070,Model1 ACC:86.8286%,Model2 Loss:0.0112,Model2 ACC:81.6571%,Blending_Model ACC:88.1429\nval, Model1 Loss:0.0060,Model1 ACC:89.2000%,Model2 Loss:0.0108,Model2 ACC:84.1333%,Blending_Model ACC:90.0667\nEpoch 11/39\n----------\ntrain, Model1 Loss:0.0066,Model1 ACC:87.5429%,Model2 Loss:0.0109,Model2 ACC:81.8857%,Blending_Model ACC:88.5714\nval, Model1 Loss:0.0063,Model1 ACC:89.3333%,Model2 Loss:0.0106,Model2 ACC:82.8000%,Blending_Model ACC:88.8000\nEpoch 12/39\n----------\ntrain, Model1 Loss:0.0066,Model1 ACC:88.1714%,Model2 Loss:0.0110,Model2 ACC:82.5714%,Blending_Model ACC:89.2286\nval, Model1 Loss:0.0063,Model1 ACC:89.1333%,Model2 Loss:0.0103,Model2 ACC:85.0000%,Blending_Model ACC:90.1333\nEpoch 13/39\n----------\ntrain, Model1 Loss:0.0067,Model1 ACC:87.8857%,Model2 Loss:0.0110,Model2 ACC:82.1143%,Blending_Model ACC:88.5143\nval, Model1 Loss:0.0069,Model1 ACC:88.0000%,Model2 Loss:0.0105,Model2 ACC:83.5333%,Blending_Model ACC:89.4000\nEpoch 14/39\n----------\ntrain, Model1 Loss:0.0068,Model1 ACC:87.9143%,Model2 Loss:0.0108,Model2 ACC:82.4000%,Blending_Model ACC:88.5714\nval, Model1 Loss:0.0071,Model1 ACC:87.9333%,Model2 Loss:0.0106,Model2 ACC:83.0667%,Blending_Model ACC:88.5333\nEpoch 15/39\n----------\ntrain, Model1 Loss:0.0069,Model1 ACC:87.5143%,Model2 Loss:0.0108,Model2 ACC:82.6000%,Blending_Model ACC:88.9143\nval, Model1 Loss:0.0066,Model1 ACC:89.8667%,Model2 Loss:0.0105,Model2 ACC:84.1333%,Blending_Model ACC:90.0000\nEpoch 16/39\n----------\ntrain, Model1 Loss:0.0069,Model1 ACC:88.6571%,Model2 Loss:0.0108,Model2 ACC:81.6286%,Blending_Model ACC:89.0286\nval, Model1 Loss:0.0072,Model1 ACC:87.4667%,Model2 Loss:0.0107,Model2 ACC:83.9333%,Blending_Model ACC:87.9333\nEpoch 17/39\n----------\ntrain, Model1 Loss:0.0069,Model1 ACC:88.2857%,Model2 Loss:0.0109,Model2 ACC:82.0286%,Blending_Model ACC:88.5143\nval, Model1 Loss:0.0070,Model1 ACC:88.8667%,Model2 Loss:0.0103,Model2 ACC:83.5333%,Blending_Model ACC:88.9333\nEpoch 18/39\n----------\ntrain, Model1 Loss:0.0068,Model1 ACC:88.5429%,Model2 Loss:0.0110,Model2 ACC:81.5143%,Blending_Model ACC:88.2857\nval, Model1 Loss:0.0075,Model1 ACC:87.3333%,Model2 Loss:0.0108,Model2 ACC:83.0000%,Blending_Model ACC:88.1333\nEpoch 19/39\n----------\ntrain, Model1 Loss:0.0071,Model1 ACC:88.2000%,Model2 Loss:0.0108,Model2 ACC:82.2571%,Blending_Model ACC:88.2571\nval, Model1 Loss:0.0075,Model1 ACC:87.6000%,Model2 Loss:0.0105,Model2 ACC:84.0667%,Blending_Model ACC:88.8000\nEpoch 20/39\n----------\ntrain, Model1 Loss:0.0072,Model1 ACC:87.5429%,Model2 Loss:0.0110,Model2 ACC:81.4286%,Blending_Model ACC:87.8571\nval, Model1 Loss:0.0069,Model1 ACC:89.0667%,Model2 Loss:0.0107,Model2 ACC:83.0000%,Blending_Model ACC:88.9333\nEpoch 21/39\n----------\ntrain, Model1 Loss:0.0073,Model1 ACC:87.5714%,Model2 Loss:0.0110,Model2 ACC:81.7143%,Blending_Model ACC:88.4571\nval, Model1 Loss:0.0070,Model1 ACC:89.4000%,Model2 Loss:0.0105,Model2 ACC:83.7333%,Blending_Model ACC:89.4667\nEpoch 22/39\n----------\ntrain, Model1 Loss:0.0072,Model1 ACC:87.8571%,Model2 Loss:0.0108,Model2 ACC:82.0000%,Blending_Model ACC:87.6286\nval, Model1 Loss:0.0075,Model1 ACC:88.2000%,Model2 Loss:0.0102,Model2 ACC:84.5333%,Blending_Model ACC:89.2000\nEpoch 23/39\n----------\ntrain, Model1 Loss:0.0072,Model1 ACC:87.7714%,Model2 Loss:0.0110,Model2 ACC:81.7429%,Blending_Model ACC:87.9429\nval, Model1 Loss:0.0074,Model1 ACC:88.1333%,Model2 Loss:0.0105,Model2 ACC:83.2000%,Blending_Model ACC:88.5333\nEpoch 24/39\n----------\ntrain, Model1 Loss:0.0071,Model1 ACC:88.1429%,Model2 Loss:0.0109,Model2 ACC:82.4286%,Blending_Model ACC:88.2000\nval, Model1 Loss:0.0076,Model1 ACC:86.8667%,Model2 Loss:0.0105,Model2 ACC:83.6667%,Blending_Model ACC:88.2667\nEpoch 25/39\n----------\ntrain, Model1 Loss:0.0074,Model1 ACC:88.0000%,Model2 Loss:0.0109,Model2 ACC:82.5143%,Blending_Model ACC:88.3714\nval, Model1 Loss:0.0074,Model1 ACC:88.8000%,Model2 Loss:0.0108,Model2 ACC:82.6000%,Blending_Model ACC:88.0667\nEpoch 26/39\n----------\ntrain, Model1 Loss:0.0076,Model1 ACC:87.5714%,Model2 Loss:0.0111,Model2 ACC:81.4286%,Blending_Model ACC:87.6571\nval, Model1 Loss:0.0074,Model1 ACC:86.8667%,Model2 Loss:0.0105,Model2 ACC:83.8000%,Blending_Model ACC:87.8000\nEpoch 27/39\n----------\ntrain, Model1 Loss:0.0075,Model1 ACC:87.8000%,Model2 Loss:0.0109,Model2 ACC:82.5714%,Blending_Model ACC:88.2857\nval, Model1 Loss:0.0078,Model1 ACC:86.1333%,Model2 Loss:0.0109,Model2 ACC:81.4000%,Blending_Model ACC:86.4667\nEpoch 28/39\n----------\ntrain, Model1 Loss:0.0074,Model1 ACC:88.0857%,Model2 Loss:0.0107,Model2 ACC:83.2571%,Blending_Model ACC:88.8000\nval, Model1 Loss:0.0079,Model1 ACC:86.7333%,Model2 Loss:0.0104,Model2 ACC:83.8000%,Blending_Model ACC:86.6667\nEpoch 29/39\n----------\ntrain, Model1 Loss:0.0073,Model1 ACC:88.0286%,Model2 Loss:0.0110,Model2 ACC:81.9714%,Blending_Model ACC:88.3143\nval, Model1 Loss:0.0080,Model1 ACC:85.6000%,Model2 Loss:0.0108,Model2 ACC:82.4667%,Blending_Model ACC:86.4000\nEpoch 30/39\n----------\ntrain, Model1 Loss:0.0077,Model1 ACC:86.8857%,Model2 Loss:0.0110,Model2 ACC:81.4571%,Blending_Model ACC:87.3714\nval, Model1 Loss:0.0075,Model1 ACC:87.7333%,Model2 Loss:0.0109,Model2 ACC:82.7333%,Blending_Model ACC:89.1333\nEpoch 31/39\n----------\ntrain, Model1 Loss:0.0077,Model1 ACC:87.3143%,Model2 Loss:0.0110,Model2 ACC:81.6857%,Blending_Model ACC:87.4000\nval, Model1 Loss:0.0075,Model1 ACC:87.6667%,Model2 Loss:0.0105,Model2 ACC:83.3333%,Blending_Model ACC:88.2000\nEpoch 32/39\n----------\ntrain, Model1 Loss:0.0076,Model1 ACC:87.4000%,Model2 Loss:0.0110,Model2 ACC:81.2857%,Blending_Model ACC:87.9143\nval, Model1 Loss:0.0078,Model1 ACC:87.0000%,Model2 Loss:0.0103,Model2 ACC:84.1333%,Blending_Model ACC:88.7333\nEpoch 33/39\n----------\ntrain, Model1 Loss:0.0075,Model1 ACC:87.8286%,Model2 Loss:0.0109,Model2 ACC:82.1714%,Blending_Model ACC:87.6000\n", "name": "stdout"}, {"output_type": "stream", "text": "val, Model1 Loss:0.0084,Model1 ACC:85.3333%,Model2 Loss:0.0107,Model2 ACC:82.6000%,Blending_Model ACC:86.7333\nEpoch 34/39\n----------\ntrain, Model1 Loss:0.0073,Model1 ACC:88.2286%,Model2 Loss:0.0109,Model2 ACC:81.6857%,Blending_Model ACC:88.1714\nval, Model1 Loss:0.0084,Model1 ACC:85.8667%,Model2 Loss:0.0105,Model2 ACC:83.9333%,Blending_Model ACC:88.2667\nEpoch 35/39\n----------\ntrain, Model1 Loss:0.0079,Model1 ACC:86.4571%,Model2 Loss:0.0109,Model2 ACC:82.1429%,Blending_Model ACC:87.1143\nval, Model1 Loss:0.0075,Model1 ACC:88.1333%,Model2 Loss:0.0104,Model2 ACC:83.4667%,Blending_Model ACC:88.9333\nEpoch 36/39\n----------\ntrain, Model1 Loss:0.0077,Model1 ACC:86.8000%,Model2 Loss:0.0111,Model2 ACC:81.3714%,Blending_Model ACC:87.2286\nval, Model1 Loss:0.0078,Model1 ACC:87.4000%,Model2 Loss:0.0106,Model2 ACC:82.7333%,Blending_Model ACC:88.1333\nEpoch 37/39\n----------\ntrain, Model1 Loss:0.0077,Model1 ACC:87.1143%,Model2 Loss:0.0110,Model2 ACC:82.0000%,Blending_Model ACC:87.5143\nval, Model1 Loss:0.0077,Model1 ACC:88.3333%,Model2 Loss:0.0105,Model2 ACC:83.4000%,Blending_Model ACC:88.4000\nEpoch 38/39\n----------\ntrain, Model1 Loss:0.0078,Model1 ACC:87.2571%,Model2 Loss:0.0109,Model2 ACC:81.9143%,Blending_Model ACC:87.9143\nval, Model1 Loss:0.0078,Model1 ACC:87.5333%,Model2 Loss:0.0103,Model2 ACC:83.6000%,Blending_Model ACC:89.4667\nEpoch 39/39\n----------\ntrain, Model1 Loss:0.0076,Model1 ACC:88.1429%,Model2 Loss:0.0110,Model2 ACC:82.1143%,Blending_Model ACC:87.9714\nval, Model1 Loss:0.0082,Model1 ACC:87.2667%,Model2 Loss:0.0105,Model2 ACC:82.4667%,Blending_Model ACC:88.2667\nTraining complete in 16m 4s\nBest val Acc: 90.133331\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "\u5c06\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u4fdd\u5b58\u4e0b\u6765\u3002"}, {"metadata": {"trusted": false}, "cell_type": "code", "source": "# torch.save(model_ft.state_dict(), './model.pth')\ntorch.save({\n            'model_1_state_dict': model_1.state_dict(),\n            'model_2_state_dict': model_2.state_dict()\n            }, './vgg-resnet-50-torch.pth')", "execution_count": 22, "outputs": []}, {"metadata": {"trusted": false}, "cell_type": "code", "source": "model_1 = models.vgg16(pretrained = True)\nmodel_2 = models.resnet50(pretrained = True)\n\nfor param in model_1.parameters():\n    param.requires_grad = False\nmodel_1.classifier = torch.nn.Sequential(torch.nn.Linear(25088,4096),\n                                        torch.nn.ReLU(),\n                                        torch.nn.Dropout(p = 0.5),\n                                        torch.nn.Linear(4096,4096),\n                                        torch.nn.ReLU(),\n                                        torch.nn.Dropout(p = 0.5),\n                                        torch.nn.Linear(4096,10))\nfor param in model_2.parameters():\n    param.requires_grad = False\nnum_ftrs = model_2.fc.in_features\nmodel_2.fc = torch.nn.Linear(num_ftrs,10)\n\nload_name = './model_cross.pth'\ncheckpoint = torch.load(load_name,map_location ='cpu')\nmodel_1.load_state_dict(checkpoint['model_1_state_dict'])\nmodel_2.load_state_dict(checkpoint['model_2_state_dict'])\nmodel_1.eval()", "execution_count": 22, "outputs": [{"output_type": "execute_result", "execution_count": 22, "data": {"text/plain": "VGG(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace)\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace)\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (6): ReLU(inplace)\n    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): ReLU(inplace)\n    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace)\n    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (13): ReLU(inplace)\n    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (15): ReLU(inplace)\n    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (18): ReLU(inplace)\n    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (20): ReLU(inplace)\n    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (22): ReLU(inplace)\n    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (25): ReLU(inplace)\n    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (27): ReLU(inplace)\n    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (29): ReLU(inplace)\n    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (classifier): Sequential(\n    (0): Linear(in_features=25088, out_features=4096, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.5)\n    (3): Linear(in_features=4096, out_features=4096, bias=True)\n    (4): ReLU()\n    (5): Dropout(p=0.5)\n    (6): Linear(in_features=4096, out_features=10, bias=True)\n  )\n)"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "### \u5c06\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u4fdd\u5b58\u81f3OBS\n\u5c06\u6a21\u578b\u4fdd\u5b58\u5230OBS\u6876\u4e2dmodel\u6587\u4ef6\u5939\u4e0b\uff0c\u4e3a\u540e\u7eed\u63a8\u7406\u6d4b\u8bd5\u3001\u6a21\u578b\u63d0\u4ea4\u505a\u51c6\u5907\u3002\u5c06\u5982\u4e0b\u4ee3\u7801\u4e2d\"obs-aifood-baseline\"\u4fee\u6539\u6210\u60a8OBS\u6876\u7684\u540d\u79f0\u3002\n"}, {"metadata": {"trusted": false}, "cell_type": "code", "source": "import moxing as mox\nmox.file.copy('./vgg-resnet-50-torch.pth','s3://ai-awe-n4/model_output2/model/vgg-resnet-50-torch.pth')\nprint(\"done\")", "execution_count": 23, "outputs": [{"output_type": "stream", "text": "done\n", "name": "stdout"}]}, {"metadata": {"trusted": false}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "pytorch-1.0.0", "display_name": "Pytorch-1.0.0", "language": "python"}, "language_info": {"name": "python", "version": "3.6.4", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 2}