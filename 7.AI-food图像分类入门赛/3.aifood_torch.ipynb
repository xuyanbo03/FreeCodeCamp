{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "# aifood baseline "}, {"metadata": {}, "cell_type": "markdown", "source": "### \u672cbaseline\u91c7\u7528pytorch\u6846\u67b6\uff0c\u5e94\u7528ModelArts\u7684Notebook\u8fdb\u884c\u5f00\u53d1"}, {"metadata": {}, "cell_type": "markdown", "source": "### \u6570\u636e\u96c6\u83b7\u53d6\n\u5c06\u60a8OBS\u6876\u4e2d\u7684\u6570\u636e\u6587\u4ef6\u52a0\u8f7d\u5230\u6b64notebook\u4e2d\uff0c\u5c06\u5982\u4e0b\u4ee3\u7801\u4e2d\"obs-aifood-baseline\"\u4fee\u6539\u6210\u60a8OBS\u6876\u540d\u79f0\u3002"}, {"metadata": {"scrolled": true, "trusted": true}, "cell_type": "code", "source": "import moxing as mox\nmox.file.copy_parallel('s3://ai-awe-n4/aifood','./aifood/')\nprint(\"done\")", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "INFO:root:Using MoXing-v1.14.0-14d5d81b\nINFO:root:Using OBS-Python-SDK-3.1.2\nINFO:root:Listing OBS: 1000\nINFO:root:Listing OBS: 2000\nINFO:root:Listing OBS: 3000\nINFO:root:Listing OBS: 4000\nINFO:root:Listing OBS: 5000\nINFO:root:pid: None.\t1000/5001\nINFO:root:pid: None.\t2000/5001\nINFO:root:pid: None.\t3000/5001\nINFO:root:pid: None.\t4000/5001\nINFO:root:pid: None.\t5000/5001\n", "name": "stderr"}, {"output_type": "stream", "text": "done\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "### \u52a0\u8f7d\u4f9d\u8d56"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "\nfrom __future__ import print_function, division\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.autograd import Variable\nimport torchvision\nfrom torchvision import datasets, models, transforms\nimport time\nimport os\n", "execution_count": 2, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### \u52a0\u8f7d\u6570\u636e\u96c6\uff0c\u5e76\u5c06\u5176\u5206\u4e3a\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "dataTrans = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n#     transforms.ColorJitter(brightness=0.5, contrast=0.5, hue=0.5),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n])\n \n    # image data path\ndata_dir = './aifood/images'\nall_image_datasets = datasets.ImageFolder(data_dir, dataTrans)\n#print(all_image_datasets.class_to_idx)    \ntrainsize = int(0.8*len(all_image_datasets))\ntestsize = len(all_image_datasets) - trainsize\ntrain_dataset, test_dataset = torch.utils.data.random_split(all_image_datasets,[trainsize,testsize])\n   \nimage_datasets = {'train':train_dataset,'val':test_dataset}\n    \n\n    # wrap your data and label into Tensor\n\n    \ndataloders = {x: torch.utils.data.DataLoader(image_datasets[x],\n                                                 batch_size=64,\n                                                 shuffle=True,\n                                                 num_workers=4) for x in ['train', 'val']}\n\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n\n    # use gpu or not\nuse_gpu = torch.cuda.is_available()", "execution_count": 6, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def train_model(model, lossfunc, optimizer, scheduler, num_epochs=10):\n    start_time = time.time()\n\n    best_model_wts = model.state_dict()\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                scheduler.step()\n                model.train(True)  # Set model to training mode\n            else:\n                model.train(False)  # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0.0\n\n            # Iterate over data.\n            for data in dataloders[phase]:\n                # get the inputs\n                inputs, labels = data\n                \n\n                # wrap them in Variable\n                if use_gpu:\n                    inputs = Variable(inputs.cuda())\n                    labels = Variable(labels.cuda())\n                else:\n                    inputs, labels = Variable(inputs), Variable(labels)\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                outputs = model(inputs)\n                _, preds = torch.max(outputs.data, 1)\n                loss = lossfunc(outputs, labels)\n\n                # backward + optimize only if in training phase\n                if phase == 'train':\n                    loss.backward()\n                    optimizer.step()\n\n                # statistics\n                running_loss += loss.data\n                running_corrects += torch.sum(preds == labels.data).to(torch.float32)\n\n            epoch_loss = running_loss / dataset_sizes[phase]\n            epoch_acc = running_corrects / dataset_sizes[phase]\n\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n                phase, epoch_loss, epoch_acc))\n\n            # deep copy the model\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = model.state_dict()\n\n    elapsed_time = time.time() - start_time\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        elapsed_time // 60, elapsed_time % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n  \n    return model", "execution_count": 4, "outputs": []}, {"metadata": {"scrolled": true, "trusted": true}, "cell_type": "code", "source": "# get model and replace the original fc layer with your fc layer\nmodel_ft = models.resnet50(pretrained=True)\nnum_ftrs = model_ft.fc.in_features\nmodel_ft.fc = nn.Linear(num_ftrs, 10)\n\nif use_gpu:\n    model_ft = model_ft.cuda()\n\n    # define loss function\nlossfunc = nn.CrossEntropyLoss()\n\n    # setting optimizer and trainable parameters\n #   params = model_ft.parameters()\n # list(model_ft.fc.parameters())+list(model_ft.layer4.parameters())\n#params = list(model_ft.fc.parameters())+list( model_ft.parameters())\nparams = list(model_ft.fc.parameters())\noptimizer_ft = optim.SGD(params, lr=0.001, momentum=0.9)\n\n    # Decay LR by a factor of 0.1 every 7 epochs\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=6, gamma=0.1)\n\nmodel_ft = train_model(model=model_ft,\n                       lossfunc=lossfunc,\n                       optimizer=optimizer_ft,\n                       scheduler=exp_lr_scheduler,\n                       num_epochs=30)", "execution_count": 13, "outputs": [{"output_type": "stream", "text": "Epoch 0/29\n----------\n", "name": "stdout"}, {"output_type": "error", "ename": "KeyboardInterrupt", "evalue": "", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)", "\u001b[0;32m<ipython-input-13-b6249b500094>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m                        \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                        \u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexp_lr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                        num_epochs=30)\n\u001b[0m", "\u001b[0;32m<ipython-input-4-dcd7e88e2d2c>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, lossfunc, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0;31m# wrap them in Variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m                     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}, {"output_type": "stream", "text": "Traceback (most recent call last):\nTraceback (most recent call last):\n  File \"/home/ma-user/anaconda3/envs/Pytorch-1.0.0/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n    send_bytes(obj)\n  File \"/home/ma-user/anaconda3/envs/Pytorch-1.0.0/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n    self._send_bytes(m[offset:offset + size])\n  File \"/home/ma-user/anaconda3/envs/Pytorch-1.0.0/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n    self._send(header + buf)\n  File \"/home/ma-user/anaconda3/envs/Pytorch-1.0.0/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n    n = write(self._handle, buf)\nBrokenPipeError: [Errno 32] Broken pipe\nTraceback (most recent call last):\n  File \"/home/ma-user/anaconda3/envs/Pytorch-1.0.0/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n    send_bytes(obj)\n  File \"/home/ma-user/anaconda3/envs/Pytorch-1.0.0/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n    self._send_bytes(m[offset:offset + size])\n  File \"/home/ma-user/anaconda3/envs/Pytorch-1.0.0/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n    send_bytes(obj)\n  File \"/home/ma-user/anaconda3/envs/Pytorch-1.0.0/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n    self._send(header + buf)\n  File \"/home/ma-user/anaconda3/envs/Pytorch-1.0.0/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n    n = write(self._handle, buf)\nBrokenPipeError: [Errno 32] Broken pipe\n  File \"/home/ma-user/anaconda3/envs/Pytorch-1.0.0/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n    self._send_bytes(m[offset:offset + size])\n  File \"/home/ma-user/anaconda3/envs/Pytorch-1.0.0/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n    self._send(header + buf)\n  File \"/home/ma-user/anaconda3/envs/Pytorch-1.0.0/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n    n = write(self._handle, buf)\nBrokenPipeError: [Errno 32] Broken pipe\nTraceback (most recent call last):\n  File \"/home/ma-user/anaconda3/envs/Pytorch-1.0.0/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n    send_bytes(obj)\n  File \"/home/ma-user/anaconda3/envs/Pytorch-1.0.0/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n    self._send_bytes(m[offset:offset + size])\n  File \"/home/ma-user/anaconda3/envs/Pytorch-1.0.0/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n    self._send(header + buf)\n  File \"/home/ma-user/anaconda3/envs/Pytorch-1.0.0/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n    n = write(self._handle, buf)\nBrokenPipeError: [Errno 32] Broken pipe\n", "name": "stderr"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "torch.save(model_ft.state_dict(), './model.pth')", "execution_count": 10, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### \u6a21\u578b\u8bad\u7ec3\n\u91c7\u7528resnet50\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\u8bad\u7ec3\u6a21\u578b,\u6a21\u578b\u8bad\u7ec3\u9700\u8981\u4e00\u5b9a\u65f6\u95f4\uff0c\u7b49\u5f85\u8be5\u6bb5\u4ee3\u7801\u8fd0\u884c\u5b8c\u6210\u540e\u518d\u5f80\u4e0b\u6267\u884c\u3002"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# \u6784\u5efa\u591a\u6a21\u578b\u878d\u5408\u7ed3\u6784\nmodel_1 = models.vgg16(pretrained = True)\nmodel_2 = models.resnet50(pretrained = True)\n\n# \u8bbe\u7f6e\u6a21\u578b\u7684\u53c2\u6570\u4e0d\u9700\u8981\u8fdb\u884c\u68af\u5ea6\u4e0b\u964d\nfor param in model_1.parameters():\n    param.requires_grad = False\nmodel_1.classifier = torch.nn.Sequential(torch.nn.Linear(25088,4096),\n                                        torch.nn.ReLU(),\n                                        torch.nn.Dropout(p = 0.5),\n                                        torch.nn.Linear(4096,4096),\n                                        torch.nn.ReLU(),\n                                        torch.nn.Dropout(p = 0.5),\n                                        torch.nn.Linear(4096,10))\nfor param in model_2.parameters():\n    param.requires_grad = False\nnum_ftrs = model_2.fc.in_features\nmodel_2.fc = torch.nn.Linear(num_ftrs,10)\n\nUse_gpu = torch.cuda.is_available()\n# \u8bbe\u7f6e\u635f\u5931\u51fd\u6570\u4ee5\u53ca\u4f18\u5316\u65b9\u6cd5\nif Use_gpu:\n    model_1 = model_1.cuda()\n    model_2 = model_2.cuda()\n    \nloss_f_1 = torch.nn.CrossEntropyLoss()\nloss_f_2 = torch.nn.CrossEntropyLoss()\n\noptimizer_1 = torch.optim.Adam(model_1.classifier.parameters(), lr = 0.00001)\nparams_2 = list(model_ft.fc.parameters())\noptimizer_2 = optim.SGD(params_2, lr=0.001, momentum=0.9)\n\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_2, step_size=7, gamma=0.1)\n\n# \u8bbe\u7f6e\u4e24\u4e2a\u6a21\u578b\u878d\u5408\u7684\u6743\u91cd\u53c2\u6570\nweight_1 = 0.6\nweight_2 = 0.4", "execution_count": 8, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def train_model_cross(model_1,model_2, loss_f_1,loss_f_2, optimizer_1,optimizer_2,weight_1,weight_2, scheduler, num_epochs=10):\n    start_time = time.time()\n\n    best_model_wts_1 = model_1.state_dict()\n    best_model_wts_2 = model_2.state_dict()\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                scheduler.step()\n                model_1.train(True)\n                model_2.train(True)  # Set model to training mode\n            else:\n                model_1.train(False)\n                model_2.train(False)  # Set model to evaluate mode\n\n            running_loss_1 = 0.0\n            running_loss_2 = 0.0\n            running_corrects_1 = 0.0\n            running_corrects_2 = 0.0\n            blending_running_corrects = 0.0\n\n            # Iterate over data.\n            for data in dataloders[phase]:\n                # get the inputs\n                inputs, labels = data\n                \n\n                # wrap them in Variable\n                if use_gpu:\n                    inputs = Variable(inputs.cuda())\n                    labels = Variable(labels.cuda())\n                else:\n                    inputs, labels = Variable(inputs), Variable(labels)\n\n\n                # forward\n                y_pred_1 = model_1(inputs)\n                y_pred_2 = model_2(inputs)\n                blending_y_pred = y_pred_1 * weight_1 + y_pred_2 * weight_2\n                _, pred_1 = torch.max(y_pred_1.data,1) # \u627e\u51fa\u6bcf\u4e00\u884c\u6700\u5927\u503c\u5bf9\u5e94\u7684\u7d22\u5f15\u503c\n                _, pred_2 = torch.max(y_pred_2.data,1)\n                _, blending_y_pred = torch.max(blending_y_pred.data,1)\n                \n                optimizer_1.zero_grad()\n                optimizer_2.zero_grad()\n\n                loss_1 = loss_f_1(y_pred_1,labels)\n                loss_2 = loss_f_2(y_pred_2,labels)\n                \n\n                # backward + optimize only if in training phase\n                if phase == 'train':\n                    loss_1.backward()\n                    loss_2.backward()\n                    optimizer_1.step()\n                    optimizer_2.step()\n                    \n                # statistics\n                running_loss_1 += loss_1.data\n                running_loss_2 += loss_2.data\n                running_corrects_1 += torch.sum(pred_1 == labels.data).to(torch.float32)\n                running_corrects_2 += torch.sum(pred_2 == labels.data).to(torch.float32)\n                blending_running_corrects += torch.sum(blending_y_pred == labels.data).to(torch.float32)\n\n            \n            epoch_loss_1 = running_loss_1/dataset_sizes[phase]\n            epoch_acc_1 = running_corrects_1 * 100/dataset_sizes[phase]\n            epoch_loss_2 = running_loss_2/dataset_sizes[phase]\n            epoch_acc_2 = running_corrects_2 * 100/dataset_sizes[phase]\n            epoch_blending_acc = blending_running_corrects * 100/dataset_sizes[phase]\n\n            print('{}, Model1 Loss:{:.4f},Model1 ACC:{:.4f}%,Model2 Loss:{:.4f},Model2 ACC:{:.4f}%,Blending_Model ACC:{:.4f}'\n                  .format(phase,epoch_loss_1,epoch_acc_1,epoch_loss_2,epoch_acc_2,epoch_blending_acc))\n\n            # deep copy the model\n            if phase == 'val' and epoch_blending_acc > best_acc:\n                best_acc = epoch_blending_acc\n                best_model_wts_1 = model_1.state_dict()\n                best_model_wts_2 = model_2.state_dict()\n\n    elapsed_time = time.time() - start_time\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        elapsed_time // 60, elapsed_time % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n\n    # load best model weights\n    model_1.load_state_dict(best_model_wts_1)\n    model_2.load_state_dict(best_model_wts_2)\n  \n    return model_1,model_2", "execution_count": 18, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "model_1,model_2 = train_model_cross(\n    model_1,model_2, loss_f_1,loss_f_2, optimizer_1,optimizer_2,weight_1,weight_2, exp_lr_scheduler, num_epochs=30)", "execution_count": 19, "outputs": [{"output_type": "stream", "text": "Epoch 0/29\n----------\ntrain, Model1 Loss:0.0055,Model1 ACC:89.9250%,Model2 Loss:0.0373,Model2 ACC:10.4250%,Blending_Model ACC:89.5000\nval, Model1 Loss:0.0065,Model1 ACC:87.6000%,Model2 Loss:0.0378,Model2 ACC:10.3000%,Blending_Model ACC:86.8000\nEpoch 1/29\n----------\ntrain, Model1 Loss:0.0045,Model1 ACC:92.3250%,Model2 Loss:0.0373,Model2 ACC:10.5500%,Blending_Model ACC:91.8750\nval, Model1 Loss:0.0062,Model1 ACC:87.0000%,Model2 Loss:0.0378,Model2 ACC:10.2000%,Blending_Model ACC:87.2000\nEpoch 2/29\n----------\ntrain, Model1 Loss:0.0037,Model1 ACC:93.7750%,Model2 Loss:0.0373,Model2 ACC:10.5750%,Blending_Model ACC:93.6500\nval, Model1 Loss:0.0056,Model1 ACC:89.5000%,Model2 Loss:0.0378,Model2 ACC:9.9000%,Blending_Model ACC:88.8000\nEpoch 3/29\n----------\ntrain, Model1 Loss:0.0032,Model1 ACC:94.6750%,Model2 Loss:0.0373,Model2 ACC:10.3250%,Blending_Model ACC:94.7500\nval, Model1 Loss:0.0055,Model1 ACC:89.2000%,Model2 Loss:0.0377,Model2 ACC:10.3000%,Blending_Model ACC:88.6000\nEpoch 4/29\n----------\ntrain, Model1 Loss:0.0026,Model1 ACC:95.9500%,Model2 Loss:0.0373,Model2 ACC:10.5750%,Blending_Model ACC:95.7500\nval, Model1 Loss:0.0054,Model1 ACC:89.4000%,Model2 Loss:0.0378,Model2 ACC:9.7000%,Blending_Model ACC:88.9000\nEpoch 5/29\n----------\ntrain, Model1 Loss:0.0022,Model1 ACC:97.0000%,Model2 Loss:0.0373,Model2 ACC:10.0000%,Blending_Model ACC:96.9500\nval, Model1 Loss:0.0054,Model1 ACC:89.3000%,Model2 Loss:0.0378,Model2 ACC:10.7000%,Blending_Model ACC:89.6000\nEpoch 6/29\n----------\ntrain, Model1 Loss:0.0018,Model1 ACC:97.7750%,Model2 Loss:0.0373,Model2 ACC:10.4250%,Blending_Model ACC:97.5500\nval, Model1 Loss:0.0054,Model1 ACC:89.8000%,Model2 Loss:0.0378,Model2 ACC:9.5000%,Blending_Model ACC:89.7000\nEpoch 7/29\n----------\ntrain, Model1 Loss:0.0015,Model1 ACC:98.2000%,Model2 Loss:0.0373,Model2 ACC:10.4750%,Blending_Model ACC:98.2750\nval, Model1 Loss:0.0054,Model1 ACC:89.4000%,Model2 Loss:0.0378,Model2 ACC:10.1000%,Blending_Model ACC:88.8000\nEpoch 8/29\n----------\ntrain, Model1 Loss:0.0013,Model1 ACC:98.4250%,Model2 Loss:0.0373,Model2 ACC:10.2250%,Blending_Model ACC:98.4000\nval, Model1 Loss:0.0055,Model1 ACC:89.7000%,Model2 Loss:0.0378,Model2 ACC:10.2000%,Blending_Model ACC:89.4000\nEpoch 9/29\n----------\ntrain, Model1 Loss:0.0010,Model1 ACC:98.9000%,Model2 Loss:0.0373,Model2 ACC:10.6500%,Blending_Model ACC:98.8000\nval, Model1 Loss:0.0052,Model1 ACC:89.9000%,Model2 Loss:0.0379,Model2 ACC:10.0000%,Blending_Model ACC:90.0000\nEpoch 10/29\n----------\ntrain, Model1 Loss:0.0009,Model1 ACC:99.0500%,Model2 Loss:0.0373,Model2 ACC:9.8750%,Blending_Model ACC:99.0000\nval, Model1 Loss:0.0052,Model1 ACC:89.4000%,Model2 Loss:0.0378,Model2 ACC:10.3000%,Blending_Model ACC:89.2000\nEpoch 11/29\n----------\ntrain, Model1 Loss:0.0008,Model1 ACC:99.4750%,Model2 Loss:0.0373,Model2 ACC:10.2750%,Blending_Model ACC:99.4250\nval, Model1 Loss:0.0054,Model1 ACC:88.9000%,Model2 Loss:0.0378,Model2 ACC:10.4000%,Blending_Model ACC:89.3000\nEpoch 12/29\n----------\ntrain, Model1 Loss:0.0007,Model1 ACC:99.4000%,Model2 Loss:0.0373,Model2 ACC:10.2250%,Blending_Model ACC:99.4000\nval, Model1 Loss:0.0055,Model1 ACC:89.2000%,Model2 Loss:0.0377,Model2 ACC:10.2000%,Blending_Model ACC:89.3000\nEpoch 13/29\n----------\ntrain, Model1 Loss:0.0006,Model1 ACC:99.6500%,Model2 Loss:0.0373,Model2 ACC:10.5250%,Blending_Model ACC:99.6750\nval, Model1 Loss:0.0055,Model1 ACC:89.8000%,Model2 Loss:0.0378,Model2 ACC:9.4000%,Blending_Model ACC:89.8000\nEpoch 14/29\n----------\ntrain, Model1 Loss:0.0005,Model1 ACC:99.6750%,Model2 Loss:0.0373,Model2 ACC:10.8250%,Blending_Model ACC:99.6750\nval, Model1 Loss:0.0057,Model1 ACC:89.1000%,Model2 Loss:0.0378,Model2 ACC:9.8000%,Blending_Model ACC:89.4000\nEpoch 15/29\n----------\ntrain, Model1 Loss:0.0004,Model1 ACC:99.8750%,Model2 Loss:0.0373,Model2 ACC:10.7500%,Blending_Model ACC:99.8250\nval, Model1 Loss:0.0055,Model1 ACC:90.1000%,Model2 Loss:0.0377,Model2 ACC:10.1000%,Blending_Model ACC:90.2000\nEpoch 16/29\n----------\ntrain, Model1 Loss:0.0004,Model1 ACC:99.8000%,Model2 Loss:0.0373,Model2 ACC:10.1000%,Blending_Model ACC:99.8500\nval, Model1 Loss:0.0057,Model1 ACC:89.7000%,Model2 Loss:0.0378,Model2 ACC:10.1000%,Blending_Model ACC:89.7000\nEpoch 17/29\n----------\ntrain, Model1 Loss:0.0003,Model1 ACC:99.8250%,Model2 Loss:0.0373,Model2 ACC:10.3500%,Blending_Model ACC:99.8500\nval, Model1 Loss:0.0056,Model1 ACC:89.4000%,Model2 Loss:0.0378,Model2 ACC:9.9000%,Blending_Model ACC:89.5000\nEpoch 18/29\n----------\ntrain, Model1 Loss:0.0003,Model1 ACC:99.8500%,Model2 Loss:0.0373,Model2 ACC:10.8250%,Blending_Model ACC:99.8250\nval, Model1 Loss:0.0059,Model1 ACC:89.0000%,Model2 Loss:0.0378,Model2 ACC:10.4000%,Blending_Model ACC:88.9000\nEpoch 19/29\n----------\ntrain, Model1 Loss:0.0003,Model1 ACC:99.9500%,Model2 Loss:0.0373,Model2 ACC:10.4500%,Blending_Model ACC:99.9750\nval, Model1 Loss:0.0059,Model1 ACC:89.8000%,Model2 Loss:0.0378,Model2 ACC:10.1000%,Blending_Model ACC:89.3000\nEpoch 20/29\n----------\ntrain, Model1 Loss:0.0002,Model1 ACC:99.9250%,Model2 Loss:0.0373,Model2 ACC:10.2500%,Blending_Model ACC:99.9000\nval, Model1 Loss:0.0057,Model1 ACC:90.2000%,Model2 Loss:0.0378,Model2 ACC:10.0000%,Blending_Model ACC:89.9000\nEpoch 21/29\n----------\ntrain, Model1 Loss:0.0002,Model1 ACC:100.0000%,Model2 Loss:0.0373,Model2 ACC:10.7750%,Blending_Model ACC:100.0000\nval, Model1 Loss:0.0061,Model1 ACC:90.0000%,Model2 Loss:0.0378,Model2 ACC:9.5000%,Blending_Model ACC:89.7000\nEpoch 22/29\n----------\ntrain, Model1 Loss:0.0002,Model1 ACC:99.9500%,Model2 Loss:0.0373,Model2 ACC:10.4750%,Blending_Model ACC:99.9750\nval, Model1 Loss:0.0060,Model1 ACC:89.9000%,Model2 Loss:0.0378,Model2 ACC:9.6000%,Blending_Model ACC:89.6000\nEpoch 23/29\n----------\ntrain, Model1 Loss:0.0002,Model1 ACC:99.9750%,Model2 Loss:0.0373,Model2 ACC:11.1500%,Blending_Model ACC:100.0000\nval, Model1 Loss:0.0058,Model1 ACC:89.9000%,Model2 Loss:0.0377,Model2 ACC:10.1000%,Blending_Model ACC:89.7000\nEpoch 24/29\n----------\ntrain, Model1 Loss:0.0002,Model1 ACC:99.9750%,Model2 Loss:0.0373,Model2 ACC:10.4000%,Blending_Model ACC:99.9750\nval, Model1 Loss:0.0057,Model1 ACC:89.7000%,Model2 Loss:0.0377,Model2 ACC:10.2000%,Blending_Model ACC:89.3000\nEpoch 25/29\n----------\ntrain, Model1 Loss:0.0001,Model1 ACC:99.9750%,Model2 Loss:0.0373,Model2 ACC:10.7500%,Blending_Model ACC:99.9750\nval, Model1 Loss:0.0058,Model1 ACC:89.8000%,Model2 Loss:0.0377,Model2 ACC:10.4000%,Blending_Model ACC:89.7000\nEpoch 26/29\n----------\ntrain, Model1 Loss:0.0001,Model1 ACC:100.0000%,Model2 Loss:0.0373,Model2 ACC:10.5500%,Blending_Model ACC:100.0000\nval, Model1 Loss:0.0060,Model1 ACC:89.5000%,Model2 Loss:0.0378,Model2 ACC:11.2000%,Blending_Model ACC:89.6000\nEpoch 27/29\n----------\ntrain, Model1 Loss:0.0001,Model1 ACC:100.0000%,Model2 Loss:0.0374,Model2 ACC:10.4000%,Blending_Model ACC:100.0000\nval, Model1 Loss:0.0061,Model1 ACC:89.9000%,Model2 Loss:0.0378,Model2 ACC:10.7000%,Blending_Model ACC:89.4000\nEpoch 28/29\n----------\ntrain, Model1 Loss:0.0001,Model1 ACC:100.0000%,Model2 Loss:0.0373,Model2 ACC:10.3250%,Blending_Model ACC:100.0000\nval, Model1 Loss:0.0062,Model1 ACC:89.9000%,Model2 Loss:0.0378,Model2 ACC:10.2000%,Blending_Model ACC:89.5000\nEpoch 29/29\n----------\ntrain, Model1 Loss:0.0001,Model1 ACC:100.0000%,Model2 Loss:0.0373,Model2 ACC:10.9750%,Blending_Model ACC:100.0000\nval, Model1 Loss:0.0063,Model1 ACC:89.9000%,Model2 Loss:0.0378,Model2 ACC:10.3000%,Blending_Model ACC:89.4000\nTraining complete in 12m 1s\nBest val Acc: 90.200005\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "\u5c06\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u4fdd\u5b58\u4e0b\u6765\u3002"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# torch.save(model_ft.state_dict(), './model.pth')\ntorch.save({\n            'model_1_state_dict': model_1.state_dict(),\n            'model_2_state_dict': model_2.state_dict()\n            }, './model_cross.pth')", "execution_count": 20, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "model_1 = models.vgg16(pretrained = True)\nmodel_2 = models.resnet50(pretrained = True)\n\nfor param in model_1.parameters():\n    param.requires_grad = False\nmodel_1.classifier = torch.nn.Sequential(torch.nn.Linear(25088,4096),\n                                        torch.nn.ReLU(),\n                                        torch.nn.Dropout(p = 0.5),\n                                        torch.nn.Linear(4096,4096),\n                                        torch.nn.ReLU(),\n                                        torch.nn.Dropout(p = 0.5),\n                                        torch.nn.Linear(4096,10))\nfor param in model_2.parameters():\n    param.requires_grad = False\nnum_ftrs = model_2.fc.in_features\nmodel_2.fc = torch.nn.Linear(num_ftrs,10)\n\nload_name = './model_cross.pth'\ncheckpoint = torch.load(load_name,map_location ='cpu')\nmodel_1.load_state_dict(checkpoint['model_1_state_dict'])\nmodel_2.load_state_dict(checkpoint['model_2_state_dict'])\nmodel_1.eval()", "execution_count": 22, "outputs": [{"output_type": "execute_result", "execution_count": 22, "data": {"text/plain": "VGG(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace)\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace)\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (6): ReLU(inplace)\n    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): ReLU(inplace)\n    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace)\n    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (13): ReLU(inplace)\n    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (15): ReLU(inplace)\n    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (18): ReLU(inplace)\n    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (20): ReLU(inplace)\n    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (22): ReLU(inplace)\n    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (25): ReLU(inplace)\n    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (27): ReLU(inplace)\n    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (29): ReLU(inplace)\n    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (classifier): Sequential(\n    (0): Linear(in_features=25088, out_features=4096, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.5)\n    (3): Linear(in_features=4096, out_features=4096, bias=True)\n    (4): ReLU()\n    (5): Dropout(p=0.5)\n    (6): Linear(in_features=4096, out_features=10, bias=True)\n  )\n)"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "### \u5c06\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u4fdd\u5b58\u81f3OBS\n\u5c06\u6a21\u578b\u4fdd\u5b58\u5230OBS\u6876\u4e2dmodel\u6587\u4ef6\u5939\u4e0b\uff0c\u4e3a\u540e\u7eed\u63a8\u7406\u6d4b\u8bd5\u3001\u6a21\u578b\u63d0\u4ea4\u505a\u51c6\u5907\u3002\u5c06\u5982\u4e0b\u4ee3\u7801\u4e2d\"obs-aifood-baseline\"\u4fee\u6539\u6210\u60a8OBS\u6876\u7684\u540d\u79f0\u3002\n"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "import moxing as mox\nmox.file.copy('./model.pth','s3://ai-awe-n4/model_output/model/resnet-50-torch.pth')\nprint(\"done\")", "execution_count": 11, "outputs": [{"output_type": "stream", "text": "done\n", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "import moxing as mox\nmox.file.copy('./model_cross.pth','s3://ai-awe-n4/model_output1/model/vgg-resnet-50-torch.pth')\nprint(\"done\")", "execution_count": 23, "outputs": [{"output_type": "stream", "text": "done\n", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "pytorch-1.0.0", "display_name": "Pytorch-1.0.0", "language": "python"}, "language_info": {"name": "python", "version": "3.6.4", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 2}